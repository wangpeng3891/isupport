{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import six.moves.cPickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf-8')\n",
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "BASE_DIR = './'\n",
    "EMBEDDING_FILE = '../data/GoogleNews-vectors-negative300.bin'\n",
    "# TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
    "# TEST_DATA_FILE = BASE_DIR + 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "# num_lstm = np.random.randint(175, 275)\n",
    "# num_dense = np.random.randint(100, 150)\n",
    "# rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "# rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "num_lstm =120\n",
    "num_dense = 40\n",
    "rate_drop_lstm =.25\n",
    "rate_drop_dense =.25\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = False # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP_MME = 'lstm_mme_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)\n",
    "########################################\n",
    "## index word vectors\n",
    "########################################\n",
    "print('Indexing word vectors')\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))\n",
    "\n",
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "print('Processing text dataset')\n",
    "\n",
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stopwords=True, stem_words=True):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "\n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    #     print( 'text  1 ',text)\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    # text = text.lower().split()\n",
    "    # Return a list of words\n",
    "    return (text)\n",
    "\n",
    "############prepare  training/validation data  /only use mme\n",
    "train_processed_query_dataset_path = './data/train_processed_query.csv'\n",
    "train_processed_query_df = pd.read_csv(train_processed_query_dataset_path)\n",
    "print(\"processed train data set shape\",train_processed_query_df.shape)\n",
    "train_texts_mme_1 = list(train_processed_query_df.loc[:,'query'].map(lambda x: text_to_wordlist(x,remove_stopwords=True, stem_words=True) ))\n",
    "train_texts_mme_2 = list(train_processed_query_df.loc[:,'faq'].map(lambda x: text_to_wordlist(x,remove_stopwords=True, stem_words=True) ))\n",
    "train_labels_mme   = list(train_processed_query_df.loc[:,'match'])\n",
    "###########add the pair of (query1,query1)  sentence as positive samples\n",
    "new_positive = list(set(train_texts_mme_1))+ list(set(train_texts_mme_2))\n",
    "print('the number of set of faq and query as positive ',len(new_positive))\n",
    "train_texts_mme_add_1 = train_texts_mme_1 + new_positive\n",
    "train_texts_mme_add_2 = train_texts_mme_2 + new_positive\n",
    "train_labels_mme_add  = train_labels_mme  + [1]* len(new_positive)\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train_texts_mme_1 + train_texts_mme_2 )\n",
    "tokenizer_fname = 'tokenizer1'\n",
    "six.moves.cPickle.dump(tokenizer, open(os.path.join('./model/', tokenizer_fname), \"wb\"))\n",
    "# tokenizer = six.moves.cPickle.load(open(os.path.join('./model/', tokenizer_fname), 'rb'))\n",
    "train_sequences_mme_add_1 = tokenizer.texts_to_sequences(train_texts_mme_add_1)\n",
    "train_sequences_mme_add_2 = tokenizer.texts_to_sequences(train_texts_mme_add_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "train_data_mme_add_1 = pad_sequences(train_sequences_mme_add_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_data_mme_add_2 = pad_sequences(train_sequences_mme_add_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_labels_mme_add = np.array(train_labels_mme_add)\n",
    "print('Shape of train_data tensor:', train_data_mme_add_1.shape)\n",
    "print('Shape of train_label tensor:', train_labels_mme_add.shape)\n",
    "\n",
    "\n",
    "########################################\n",
    "## prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "tokenizer = six.moves.cPickle.load(open(os.path.join('./model/', tokenizer_fname), 'rb'))\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "#         if word =='import':\n",
    "#             print(embedding_matrix[i])\n",
    "#             2.06054688e-01   2.27539062e-01  -2.13867188e-01   6.49414062e-02\n",
    "#    4.12597656e-02   8.49609375e-02   1.48437500e-01  -2.91015625e-01\n",
    "#   -7.08007812e-02   2.02148438e-01  -1.56402588e-03  -1.19140625e-01\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "embedding_matrix_path = './model/embedding_matrix.npy'\n",
    "np.save(embedding_matrix_path,embedding_matrix)\n",
    "\n",
    "# ######################################\n",
    "# sample train/validation data\n",
    "# #######################################\n",
    "np.random.seed(1234)\n",
    "perm_mme_add = np.random.permutation(len(train_data_mme_add_1))\n",
    "idx_train_mme_add = perm_mme_add[:int(len(train_data_mme_add_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val_mme_add =  perm_mme_add[int(len(train_data_mme_add_1)*(1-VALIDATION_SPLIT)):]\n",
    "print('idx_train len', len(idx_train_mme_add))\n",
    "print('idx_val  len',len(idx_val_mme_add))\n",
    "\n",
    "data_train_mme_add_1  = np.vstack((train_data_mme_add_1[idx_train_mme_add], train_data_mme_add_2[idx_train_mme_add]))\n",
    "data_train_mme_add_2  = np.vstack((train_data_mme_add_2[idx_train_mme_add], train_data_mme_add_1[idx_train_mme_add]))\n",
    "labels_train_mme_add  = np.concatenate((train_labels_mme_add[idx_train_mme_add], train_labels_mme_add[idx_train_mme_add]))\n",
    "print(\"data_train_mme_add_1 lenth\", data_train_mme_add_1.shape[0])\n",
    "data_val_mme_add_1  = np.vstack((train_data_mme_add_1[idx_val_mme_add], train_data_mme_add_2[idx_val_mme_add]))\n",
    "data_val_mme_add_2  = np.vstack((train_data_mme_add_2[idx_val_mme_add], train_data_mme_add_1[idx_val_mme_add]))\n",
    "labels_val_mme_add  = np.concatenate((train_labels_mme_add[idx_val_mme_add], train_labels_mme_add[idx_val_mme_add]))\n",
    "print(\"labels_val_mme_add length\", labels_val_mme_add.shape[0])\n",
    "weight_val = np.ones(len(labels_val_mme_add))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val_mme_add==0] = 1.309028344\n",
    "\n",
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "########################################\n",
    "## add class weight\n",
    "########################################\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "# t2 = time.time()\n",
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "        outputs=preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='nadam',\n",
    "              metrics=['acc'])\n",
    "# model.summary()jia\n",
    "# weight_val = np.ones(len(labels_val_mme_add))\n",
    "# if re_weight:\n",
    "#     weight_val *= 0.472001959\n",
    "#     weight_val[labels_val_mme_add==0] = 1.309028344\n",
    "# STAMP_MME = 'lstm_mme_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "#         rate_drop_dense)\n",
    "print(STAMP_MME)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "bst_model_path = STAMP_MME + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_train_mme_add_1, data_train_mme_add_2], labels_train_mme_add, \\\n",
    "                 validation_data=([data_val_mme_add_1, data_val_mme_add_2], labels_val_mme_add, weight_val), \\\n",
    "                 epochs=200, batch_size=2048, shuffle=True, \\\n",
    "                 class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "###########predict on the mme data\n",
    "TEST_DATA_FILE2 = './data/predict_processed_query.csv'\n",
    "pre_processed_query_df = pd.read_csv(TEST_DATA_FILE2)\n",
    "pre_texts_1 = list(pre_processed_query_df.loc[:,'query'].map(lambda x: text_to_wordlist(x,remove_stopwords=True, stem_words=True) ))\n",
    "pre_texts_2 = list(pre_processed_query_df.loc[:,'faq'].map(lambda x: text_to_wordlist(x,remove_stopwords=True, stem_words=True) ))\n",
    "\n",
    "# print(test_texts_1)\n",
    "pre_sequences_1 = tokenizer.texts_to_sequences(pre_texts_1)\n",
    "pre_sequences_2 = tokenizer.texts_to_sequences(pre_texts_2)\n",
    "print(pre_sequences_1[0])\n",
    "\n",
    "\n",
    "pre_data_1 = pad_sequences(pre_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pre_data_2 = pad_sequences(pre_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', pre_data_1.shape)\n",
    "############compute socre for every pair of faq and query\n",
    "import time\n",
    "t1 = time.time()\n",
    "preds1 = model.predict([pre_data_1, pre_data_2], batch_size=8192, verbose=1)\n",
    "preds2= model.predict([pre_data_2, pre_data_1], batch_size=8192, verbose=1)\n",
    "preds =(preds1 +preds2)/2\n",
    "pre_processed_query_df.loc[:,'score'] =preds\n",
    "pre_processed_query_df.to_csv(\"./model/result_query_df_train_mme600.csv\")\n",
    "\n",
    "anser_index = pre_processed_query_df.groupby('query').apply(lambda subf: subf.loc[:,'score'].argmax())\n",
    "anser_index2 = list(anser_index)\n",
    "currectnum = pre_processed_query_df.loc[list(anser_index),'match'].sum()\n",
    "t2 = time.time()\n",
    "print(list(anser_index))\n",
    "print('currectnum------------------------',currectnum)\n",
    "print(pre_processed_query_df.loc[list(anser_index),:])\n",
    "print(\"time of prediction for each query\",(t2-t1)/len(anser_index2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
